import json
import boto3
import pandas as pd
from io import StringIO
from datetime import datetime
def album_func(data_json):
    album_history=[]
    for i in data_json['items']:
        aid = i['track']['album']['id']
        aname = i['track']['album']['name']
        areleasedate = i['track']['album']['release_date']
        atotaltracks = i['track']['album']['total_tracks']
        aurl = i['track']['album']['external_urls']['spotify']
        aele={'album_id':aid,'name':aname,'release_date':areleasedate,'Total_Tracks':atotaltracks,"URL":aurl}
        album_history.append(aele)
    return album_history
    
def artist_func(data_json):
    artist_history=[]
    for i in data_json['items']:
        i1=len(i['track']['artists'])
        for j in range(i1):
            aid=i['track']['artists'][j]['id']
            aname=i['track']['artists'][j]['name']
            aurl=i['track']['artists'][j]['external_urls']['spotify']
            aele={'artist_id':aid,'artist_name':aname,'artist_url':aurl}
            artist_history.append(aele)
    return artist_history
    
def song_func(data_json):
    song_history = []
    for i in data_json['items']:
        song_id = i['track']['id']
        song_name = i['track']['name']
        song_duration = i['track']['duration_ms']
        song_url = i['track']['external_urls']['spotify']
        song_popularity = i['track']['popularity']
        song_added = i['added_at']
        album_id = i['track']['album']['id']
        artist_id = i['track']['album']['artists'][0]['id']
        sele = {'song_id':song_id,'song_name':song_name,'duration_ms':song_duration,'url':song_url,
                        'popularity':song_popularity,'song_added':song_added,'album_id':album_id,
                        'artist_id':artist_id
                       }
        song_history.append(sele)
        return song_history
        
def lambda_handler(event, context):
    cb=boto3.client('s3')
    Bucket="spotify-bucket-phani"
    Key="raw_data/tobe_processed/"
    spotify_data=[]
    spotify_filename=[]
    #Collect the filenames from S3
    '''
    {
'ResponseMetadata': {'RequestId': 'SE1GM8A4G9VQ8JWK', 'HostId': 'vtk3JS+aY1vTvuggRHp39Iwp+PpIGzv5mssmvT57HLq8pn2sHxTiFTjWl2k/c/Mo3I2Z26AsWmWy+FY6Sk3IQs4gA3O9X
W0AgwHnjTj32qk=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'vtk3JS+aY1vTvuggRHp39Iwp+PpIGzv5mssmvT57HLq8pn2sHxTiFTjWl2k/c/Mo3I2Z26AsWmWy+FY6Sk3I
Qs4gA3O9XW0AgwHnjTj32qk=', 'x-amz-request-id': 'SE1GM8A4G9VQ8JWK', 'date': 'Wed, 22 May 2024 21:03:30 GMT', 
'x-amz-bucket-region': 'us-east-1', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 
'RetryAttempts': 0}, 
'IsTruncated': False, 'Marker': '', 
'Contents': [{'Key': 'raw_data/tobe_processed/', 'LastModified': datetime.datetime(2024, 5, 22, 19, 22, 16, tzinfo=tzlocal()), 
'ETag': '"d41d8cd98f00b204e9800998ecf8427e"', 'Size': 0, 'StorageClass': 'STANDARD', 
'Owner': {'DisplayName': 'phanikulkarni7', 'ID': 'b993e12924c8506cc4506b2df17d4805281e6036188604c1c3b8dbad2c590f4d'}}, 
{'Key': 'raw_data/tobe_processed/spotify_raw_file2024-05-22 20:18:31.379689.json', 
'LastModified': datetime.datetime(2024, 5, 22, 20, 18, 32, tzinfo=tzlocal()), 'ETag': '"afd5220e9b5c3e37c9f4430e9e112dda"', 
'Size': 228580, 'StorageClass': 'STANDARD', 
'Owner': {'DisplayName': 'phanikulkarni7', 'ID': 'b993e12924c8506cc4506b2df17d4805281e6036188604c1c3b8dbad2c590f4d'}}, 
{'Key': 'raw_data/tobe_processed/spotify_raw_file2024-05-22 20:25:47.104327.json', 
'LastModified': datetime.datetime(2024, 5, 22, 20, 25, 48, tzinfo=tzlocal()), 'ETag': '"afd5220e9b5c3e37c9f4430e9e112dda"', 
'Size': 228580, 'StorageClass': 'STANDARD',
 'Owner': {'DisplayName': 'phanikulkarni7', 'ID': 'b993e12924c8506cc4506b2df17d4805281e6036188604c1c3b8dbad2c590f4d'}}], 
 'Name': 'spotify-bucket-phani', 'Prefix': 'raw_data/tobe_processed/', 'MaxKeys': 1000, 'EncodingType': 'url'}
    '''
    for i in cb.list_objects(Bucket=Bucket,Prefix=Key)['Contents']:
        file_name=i['Key']
        if(file_name.split('.')[-1]=="json"):
            res=cb.get_object(Bucket=Bucket, Key=file_name)
            content=res["Body"]
            json_ob=json.loads(content.read())
            spotify_data.append(json_ob)
            spotify_filename.append(file_name)
    
    #Transformation code        
    for data in spotify_data:
        album_history=album_func(data)
        artist_history=artist_func(data)
        song_history=song_func(data)
        
        album_dataframe = pd.DataFrame.from_dict(album_history)
        artists_dataframe = pd.DataFrame.from_dict(artist_history)
        song_dataframe = pd.DataFrame.from_dict(song_history)
        
        album_dataframe['release_date']=pd.to_datetime(album_dataframe['release_date'])
        song_dataframe['song_added']=pd.to_datetime(song_dataframe['song_added'])
        
        #Directly dataframe cannot be moved to S3. It accepts only objects
        songs_filename ="transformed_data/songs_data/song_transformed"+str(datetime.now())+".csv"
        song_buffer=StringIO()
        song_dataframe.to_csv(song_buffer,index=False)
        song_content=song_buffer.getvalue()
        cb.put_object(Bucket="spotify-bucket-phani", Key=songs_filename, Body=song_content)
        
        album_filename="transformed_data/album_data/album_transformed"+str(datetime.now())+".csv"
        album_buffer=StringIO()
        album_dataframe.to_csv(album_buffer,index=False)
        album_content=album_buffer.getvalue()
        cb.put_object(Bucket="spotify-bucket-phani",Key=album_filename,Body=album_content)
        
        artist_filename="transformed_data/artists_data/artist_transformed"+str(datetime.now())+".csv"
        artist_buffer=StringIO()
        artists_dataframe.to_csv(artist_buffer,index=False)
        artist_content=artist_buffer.getvalue()
        cb.put_object(Bucket="spotify-bucket-phani",Key=artist_filename,Body=artist_content)
    
    #Copy the processed files from raw_data/tobe_processed folder to raw_data/processed and then delete the files present in raw_data/tobe_processed
    s3_resource = boto3.resource('s3')
    for filename in spotify_filename:
        copy_resource={'Bucket':Bucket, 'Key':filename}
        #s3_resource.meta.client.copy(copy_source, target_bucket, transformed_filename='raw_data/tobe_processed/spotify_raw_file2024-05-22 21:17:08.085584.json')
        s3_resource.meta.client.copy(copy_resource,Bucket,'raw_data/processed_data/'+filename.split('/')[-1])
        s3_resource.Object(Bucket,filename).delete()
